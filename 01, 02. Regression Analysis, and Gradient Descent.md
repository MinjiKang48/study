# 01, 02. Regression Analysis, and Gradient Descent

---

#### 1. machine learning이란?

- Arthur Samuel(1959)
  - 컴퓨터에게 명확한 프로그램을 하지 않고 학습 능력을 제공하는 연구 분야
- Tom Michel(1999)
  - *"***A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."**

- learning algorithms의 유형
  - Supervised learning
    - 컴퓨터에 무언가 하는 방법을 가르쳐 주고 그 방법을 사용하도록 한다; 그 방법을 사용하여 새로운 지식을 얻는다.
  - Unsupervised learning
    - 컴퓨터가 무언가 하는 방법을 배울 수 있도록 한다. 그 방법으로 데이터에서의 구조와 패턴을 결정한다.
  - Reinforcement learning
  - Recommender systems

---

#### 2. Supervised learning - introduction

- 머신러닝의 가장 많은 문제 유형

  - ex) 집 값 예상

    - 집 값에 대한 데이터와 feet 사이즈와 얼마나 연관되어 있는 정도의 데이터 수집
    - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image.png)

    - 예상 문제!!
      - 이러한 데이터가 주어지고, 친구가 750$feet^2$의 집을 가지고 있다고 하면 얼마의 집 값을 예상할 수 있을까?

  - 접근 방법

    - 데이터를 통과한 직선
      - $ 150,000
    - 2차식
      - $ 200,000
    - (나중에 다룰 것)
      - 어떻게 직선인지 곡선인지를 결정할 것인가
    - 이러한 접근 방법들이 supervised learning을 하는 방법을 나타낸다

- 의미!!

  - 우리는 알고리즘에 "정답"이 제공되는 데이터 셋을 제공한다
  - 그래서 우리는 집에 대한 실제 가격을 알 수 있다.
    - *training data*에서 가격을 확실한 값으로 만드는 것을 배울 수 있다!
    - 그리고 알고리즘은 가격을 모르는 새로운 training data를 기반으로 더 정확한 정답을 만들어야 한다
    - *가격 예상*

- 또 이러한 문제를 **regression problem**이라고 한다.

  - 연속적인 출력값을 예측
  - 실제 이산 묘사 없음

- 또 다른 예시

  - 유방 암이 악성 암인지 양성 암인지 종양의 크기로 구분
  - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[1].png)
  - 데이터 살펴보기
    - 각 5개 존재
    - 종양의 크기로 예후를 측정할 수 있을까?
    - 이건 **classification problem**의 예시
      - 두 개의 이산적인 클래스에서 하나로 데이터를 분류하는 것
        - 두 클래스의 사이에 존재하는 것은 없음, 단순히 악성이나 아니냐
      - classification problem은 출력에 대해서 가능한 값의 이산 수를 갖는다.
        - 즉, 만약 4개의 값을 갖는다면
          - 0  "양성"
          - 1  "유형 1"
          - 2  "유형 2"
          - 3  "유형 4"
  - classification problems는 다른 방법으로 데이터를 plotting할 수 있다.
    - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[2].png)
      - 하나의 속성을 사용한 경우(size)

- 다른 문제에서는 여러 개의 attributes를 사용할 것

  - 예를 들어, 나이와 종양의 사이즈를 알고 있는 경우
    - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[3].png)

- 이 데이터를 기반으로 class를 나눌 수 있다.

  1. 두 개의 그룹 사이에 직선을 그린다.
  2. 두 그룹을 정의하기 위해서 더 복잡한 함수를 사용한다. (나중에 다룰 것)
  3. 개개인의 종양 사이즈와 나이를 알고 있을 때, class를 나누기 위한 정보로 사용할 수 있다.

- 고려해 볼 많은 features가 있을 수 있다.

  1. 종양의 두께
  2. 세포 크기의 균일성
  3. 세포 모양의 균일성

- 가장 exciting한 알고리즘은 무한개의 features를 다룰 수 있다.

  - 어떻게 무한개의 features를 다룰 수 있을까?
    - support vector machine으로 수학적 트릭을 간단하게 만든다 (나중에 다룰 것)
      - 만약 무한대로 긴 리스트를 가지고 있다면, 리스트를 가지고 알고리즘을 발전시킬 수 있다.

- *요약*

  - Supervised learning은 "올바른" 데이터를 갖도록 한다
    - Regression problem과 Classification problem에서

---

#### 3. Unsupervised learning - introduction

- Supervised learning 다음으로 자주 다뤄지는 문제 유형

- Unsupervised learning에서는 label이 붙지 않은 데이터를 갖는다.

  - 구조지을 수 있는 데이터 셋

- 구조짓는 방법의 하나는 데이터를 그룹으로 clustering하는 것이다.

  - **Clustering algorithm**

- *Clustering algorithm*

  - clustering algorithm의 예시
    - 구글 뉴스
      - 뉴스 스토리를 응집력 있는 그룹끼리 생성
    - 다른 문제들에서도 사용된다
      - 유전체학
      - microarray 데이터
        - 각 array마다 한 그룹을 가지고 있다.
        - 각 유전자의 척도마다
        - 개인을 어떤 유형의 사람으로 clustering하는 알고리즘을 실행
        - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[4].png)
      - 컴퓨터 clusters 구성하기
        - 잠재적 약점을 식별하거나 업무량을 효율적으로 분산
      - 소셜 네트워크 분석
        - 고객 정보
      - 우주 데이터 분석
        - 알고리즘은 놀라운 결과를 준다.
  - 기본적으로
    - 알아서 구조를 생성할 수 있는가?
    - 답을 제공하지 않기 때문에, unsupervised learning을 해야 한다

- *Cocktail party algorithm*

  - cocktail party problem

    - 많은 목소리가 섞여있다.

      - 모두가 말하는 것을 듣기가 어려움
      - 두 사람이 말하고 있다.
      - 마이크와 사람의 거리는 다르다
      - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[5].png)

    - 마이크가 어디에 있느냐에 따라 대화의 약간씩 다른 내용을 기록한다.

      - 목소리가 겹치는 것도 적지 않게 나타남

    - 각 마이크에서의 대화 기록을 가지고

      - cocktail party algorithm을 사용한다.

      - 알고리즘은 오디오 기록을 처리한다.

        1. 두 개의 오디오 소스가 있음을 결정

        2. 두 개의 소스를 분리한다.

    - 복잡한 문제인가?

      - 하나의 코드로 알고리즘 실행이 가능하다
      - $${[W, s, v] = svd((repmat(sum(x.*x, 1), size(x,1),1).*x)*x');}$$ -> octave/MATLAB
        - 식별하기 어렵지만 프로그램은 짧ek

  - 알고리즘 이해

    - svd 
      - octave에 내장된 선형대수학
      - C++에서는 더 복잡

---

#### 4. Linear Regression

- 집 값 예상하는 예제
  - Supervised learning regression problem
- 무슨 값으로 시작??
  - training set = data set
  - 표기
    - m = training examples의 수
    - x's = input 변수 / features
    - y's = output 변수 "target" 변수
      - $(x,y)$ : 하나의 training example
      - $(x^i, y^j)$ : 특정 example ($i^{th}$ training example)
        - i는 training set의 index
- 정의된 training set을 가지고 어떻게 사용할 것인가
  - training set을 learning algorithm에 통과시킴
  - 알고리즘의 출력은 함수 $h$ (h = hypothesis)
    - 이 함수는 input을 가지고 동작 (새로운 집의 크기)
    - 추정된 y 값을 출력하려고 노력함
- hypothesis $h$는 어떻게 나타내는 가?
  - $h_\theta = \theta_0 + \theta_1x$
  - 간단하게 $h(x)$
- $h$함수의 의미
  - x에 대한 선형 함수 값은 y라는 의미
  - $\theta_i$는 파라미터
    - $\theta_0$는 0의 상태
    - $\theta_1$는 gradient
- 이런 함수의 종류는 하나의 변수를 갖는 linear regression
  - *univariate linear regression*
- 요약
  - hypothesis는 몇몇의 변수를 갖는다 ($\theta_0, \theta_1, x$)
  - learning system에 의해 결정된 parameters를 사용한다 ($\theta_i$)
  - input에 대한 예측을 출력한다

---

#### 5. Linear regression - implementation (cost function)

- cost function은 가지고 있는 데이터에 가장 잘 맞는 직선을 찾아내도록 한다
- $\theta_i$(파라미터) 값을 선택
  - 다른 값은 다른 함수를 만든다
  - 만약 $\theta_0 = 1.5$이고 $\theta_1 = 0$이면 $1.5 @ y$에 따른 X에 대해 평행한 직선을 갖는다
  - 만약 $\theta_1 > 0$이면 양수 기울기를 갖는다.
- training set에 기반하여 직선을 만드는 파라미터를 생성하고 싶다
  - 이러한 파라미터를 선택하여 $h_\theta(x)$가 training examples의 y와 비슷함
    - 기본적으로, 가능한 실제 y값과 비슷한 출력을 제공하는 $h_\theta(x)$에 training set의 x들을 사용한다.
    - $h_\theta(x)$가 *y의 모방*이라고 생각해보자
      - 이 함수는 x를 y로 바뀌도록 한다
      - 이미 y를 가지고 있음을 고려하고 $h_\theta(x)$가 x를 y로 바꾸는 일을 얼마나 잘 하는지 평가할 수 있다
- 공식화
  - 우리는 *minimization problem*을 해결하고 싶다
  - $(h_\theta(x) - y)^2$ 최소화
    - $h(x)$와 모든 각각의 example에 대한 y 사이의 차이를 최소화하는 것이다
  - 모든 training set에 대해 이 최소화한 차이를 합한다
  - $\frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$
    - 예상된 집 값과 실제 집 값 사이의 차이 제곱을 최소화한다
      - $\frac{1}{2m}$
        - $\frac{1}{m}$
          - 평균을 결정한다
        - $\frac{1}{2m}$
          - 약간 더 수학적 계산이 쉽도록 만든 것이며, 이미 정한 모든 상수는 변하지 않는다
          - 최솟값의 반의 여전히 최솟값!
      - $\theta_0/\theta_1$를 최소화한다는 것은 hypothesis 함수에 $\theta_0, \theta_1$를 사용했을 때 평균적으로 x의 *최소 편차*를 y에서 찾은 $\theta_0$과 $\theta_1$의 값을 얻는다는 것
  - cost function을 깔끔하게 정리한다면
    - $J(\theta_0, \theta_1) = \frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$
- cost function을 최소화하고 싶다
  - (summation항 때문에) cost function은 선천적으로 언제든지 training set에 대한 모든 데이터를 본다
- **다시 강조**
  - **Hypothesis**
    - prediction machine처럼 보임
    - x 값이 주어지고, 추정된 y 값을 얻는다
    - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[10].png)
  - **Cost**
    - training data를 사용하여 가능한 hypothesis가 정확하도록 만드는 $\theta$값을 결정하는 것이다
      - $minimize \; \theta_0,\theta_1 \; J(\theta_0, \theta_1)$
      - cost function은 squared error cost function
        - cost function은 대부분의 regression functions에 대해 적합한 선택이다
        - 아마도 흔히 쓰이는 함수이다
  - $J(\theta_0, \theta_1)$의 경우는 무슨 일을 하는지, 왜 이렇게 동작하는지 그리고 이후에 어떻게 사용될 지에 대해 약간 추상적이다.
- *Cost function - a deeper look*
  - cost function을 적용하는 방법과 왜 이 함수를 사용하는지에 대해 생각해보자
    - cost function은 파라미터를 결정한다
    - 파라미터와 관련된 값은 다른 값에 다른 결과를 생성하면서 어떻게 hypothesis가 동작하는지를 결정한다
  - 단순화한 hypothesis
    - $\theta_0 = 0$이라고 가정
    - $h_\theta(x) = \theta_1x$
    - $J(\theta_1) = \frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$
    - $minimize \; \theta_1 \; J(\theta_1)$
  - 여기서의 cost function과 목표는 $\theta_0$가 있을 때와 비슷하지만 파라미터가 좀 더 단순해졌다
    - 단순화한 hypothesis는 cost function J()을 시각화하는 게 좀 더 쉽다
  - 그래서 hypothesis는 0,0을 통과한다
  - 이해하고 싶은 두 개의 핵심 함수
    - $h_\theta(x)$
      - hypothesis는 x에 대한 함수
        - 집 크기가 몇인지에 대한 함수
    - $J(\theta_1)$
      - $\theta_1$ 파라미터에 대한 함수
    - 예제
      - $\theta_1 = 1$
      - $J(\theta_1) = 0$
    - 그래프에 그리기
      - $\theta_1$ vs $J(\theta_1)$
      - Data
        1. $\theta_1 = 1$  $J(\theta_1) = 0$
        2. $\theta_1 = 0.5$  $J(\theta_1) = $ ~0.58
        3. $\theta_1 = 0$  $J(\theta_1) = $ ~2.3
    - plot할 값의 범위를 계산한다면
      - $J(\theta_1)$ vs $\theta_1$ 다차식을 얻을 수 있다. (2차식으로 보임)
      - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[13].png)
  - learning algorithm에 대한 최적화 목적은 $J(\theta_1)$을 최소화하는 $\theta_1$의 값을 찾는 것
    - 그러므로, 여기 $\theta_1 = 1$은 $\theta_1$에 제일 좋은 값



---

#### 6. A deeper insight into the cost function - simplified cost function

- 등고선 plots 
