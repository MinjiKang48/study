# 17. Large Scale Machine Learning

#### 1. Learning with large datasets

---

###### 왜 large datasets??

- 성능 향상을 위한 좋은 방법 중 하나는 작은 bias의 알고리즘을 가지고 많은 데이터를 학습시키는 것이다.
  - 즉, 혼동이 쉬운 단어들 사이에서는 분류같은 경우
  - 알고리즘의 가능한 많은 데이터를 넣었더니 모든 성능이 거의 비슷해졌다.
    - larger datasets인 만큼 성능이 좋아진다
  - 그래서 large datasets은 학습하기에 좋다
  - **근데** large datasets은 계산 문제가 발생한다!

---

###### Learning with large datasets

- 예를 들어 m = 100,000,000인 datasets을 가지고 있다고 하자

  - 이건 현실적인 datasets
    - 인구 조사 데이터
    - 웹사이트 트래픽 데이터
  - 이런 큰 시스템을 어떻게 logistic regression에서 학습시킬 수 있을까??
    - $ \theta_j := \theta_j - \alpha\frac{1}{m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(j)})x_j^{(i)}$
      - gradient descent를 하는 한 단계마다 100,000,000항을 더해야 한다.

- 이 엄청난 합의 cost 계산 때문에 효율적인 방법을 찾으려고 함

  - 다른 접근 방법 사용
  - 합계를 피하기 위한 최적화

- 제일 먼저 해야 할 일은 *100,000,000 대신 1000 examples를 학습해도 되는가??*를 확인하는 일

  - 무작위로 작은 집합을 선택한다
  - 잘 동작하는 시스템을 개발할 수 있는가?
    - 종종 그렇다고 할 수 있음
      - 이런 경우에는 빅데이터로 인한 두통을 피할 수 있음

- 작은 샘플을 가지고 동작한다고 할 때, x축이 m (training set size)이고 y축이 error인 그래프에 plotting하면서 결론이 맞는지 확인할 수 있다

  - 만약 이런 식으로 그래프가 그려진다면

    - ![img](https://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[2].png)
    - 이건 high variance problem = overfitting 문제
      - 더 많은 examples가 성능 향상에 도움이 될 것

  - 만약 이런 식이라면

    - ![img](https://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[3].png)

    - high bias problem = underfitting

      - 더 많은 examples는 도움되지 않음 (미리 알고 있으면 시간 절약 가능)

      - 할 수 있는 해결 방법

        1. features 추가

        2. hidden units 추가 (neural network라면)

           

---

#### 2. Stochastic Gradient Descent

---

- 많은 learning algorithm에서 최적화를 목적과 cost function을 최소화하기 위해서 미분했다.
  -  large datasets을 가지고 있을 때, gradient descent는 비싸질 것이다.
  - 알고리즘을 조정할 수 있는 large data sets를 최적화하는 다른 방법을 정의할 것
- linear regression model을 gradient descent로 학습한다고 가정하자
  - Hypothesis
    - $h_\theta = \sum\limits_{j=0}^n\theta_jx_j$
  - Cost function
    - $J_{train}(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2$
  - 두 개의 파라미터 ($\theta_0, \theta_1$)와 cost function으로 plotting하면
    - ![img](https://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[6].png)
    - bowl 모양으로 그려짐
- *상기시키기*   **gradient descent가 동작하는 방법**
  - 반복
    - $ Repeat \;\{ \theta_j := \theta_j - \alpha\frac{1}{m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(j)})x_j^{(i)} (for \;every\; j = 0,...,n) \}$
  - inner loop에서 반복적으로 파라미터 $\theta$를 업데이트함
- **stochastic gradient descent** algorithm examples로 linear regression을 사용할 것
  - Logistic regression
  - Neural networks
  - 같은 다른 알고리즘에서도 적용할 수 있는 아이디어
- gradient descent를 사용하여 contour plot에서 global minimum을 찾기 위한 반복
  - <img src="https://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[8].png" alt="img"  />
- 언급한 것과 같이, m이 크다면 gradient descent는 매우 비싸질 것
- 지금까지 이걸 그냥 gradient descent라고 불렀지만, 이런 종류의 gradient descent는 **batch gradient descent**라고 부른다
  - 같은 시간에 모든 examples를 본다는 의미
- batch gradient descent는 huge datasets에 좋지 않음
  - 300,000,000 개의 기록이 있다면 모든 기록을 메모리에 저장할 수 없기 때문에 디스크에서 모든 기록을 메모리로 읽어와야 한다
    - 모든 기록을 읽음으로써, 알고리즘을 통해 한 단계를 움직일 수 있다??? (반복)
  - 모든 단계를 반복
    - 수렴하기에 오랜 시간이 걸린다는 의미
    - 특히, 디스크의 입출력은 엄청난 병목 현상이 발생, 엄청난 수의 읽기가 필요 (?????)
      - Especially because disk I/O is typically a system bottleneck anyway, and this will inevitably require a *huge* number of reads
- 한 번에 하나의 example을 봐야 하는 다른 알고리즘을 생각해 낼 것!

---

###### Stochastic gradient descent

- 약간 차이가 있는 cost function 정의
  - $cost(\theta, (x^{(i)}, y^{(i)})) = \frac{1}{2}(h_\theta(x^{(i)}) - y^{(i)})^2$
    - 특정 example $(x^i, y^i)$에 대한 $\theta$의 cost값을 나타냄
      - 그 example의 제곱 오류의 $\frac{1}{2}$배를 하여 계산
    - hypothesis가 하나의 example에 대해 얼마나 잘 동작하는지 측정
- cost function을 다시 작성할 수 있음
  - $J_{train}(\theta) = \frac{1}{m}\sum\limits_{i=1}^m cost(\theta, (x^{(i)}, y^{(i)}))$
    - 이것은 batch gradient descent cost function과 같음 
- linear regression의 약간 수정된 관점을 가지고 stochastic gradient  descent가 어떻게 동작하는지 적을 수 있다
  1. 무작위로 섞기
     - training examples를 재정렬한다
  2. 알고리즘 몸통
     - $Repeat \{\\ \;\;\;\;\;\;\;\;for \;\;i := 1,..., m \;\{\\\;\;\;\;\;\;\;\;\;\;\theta_j := \theta_j - \alpha(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \;\\\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(for \;every\; j = 0,...,n)\\\;\;\;\;\;\;\;\;\;\;\}\\\;\}$
- *여기서 무슨 일을 할까?*
  - $h_\theta(x^{(i)} - y^{(i)})x_j^{(i)}$
  - batch gradient descent에서의 합과 같음을 알 수 있다
  - 이 식은 $cost(\theta, (x^i, y^i))$의 파라미터 $\theta_j$에 대한 편미분과 같음을 보여준다
    - $\frac{\partial}{\partial \theta_j} cost(\theta, (x^{(i)}, y^{(i)}))$
- stochastic gradient descent algorithm은 각 example에서 무엇을 스캔하는가?
  - inner loop가 하는 일 - 이해되지 않음
    - 예시 1을 보면 첫 번째 training example에 대해 첫 번째 단계를 시작한다.
      - 첫 번째 training example이 끝나면, 두 번째 training example로 넘어간다
    - 이제 시도할 파라미터 공간에서 두 번째 단계를 시작하고 두 번째 training example을 더 잘 맞도록 한다.
      - 그리고 세 번째 training example로 넘어간다
    - 계속....
    - 마지막 데이터에 도달할 때까지 계속한다
  - We may now repeat this who procedure and take multiple passes over the data
- 처음에 *무작위로 셔플링하는 것*은 데이터를 랜덤 정렬한다는 의미로 편향되어서 동작하지 않는다는 것이다.
  - 랜덤화하는 것은 수렴되는 것의 속도를 약간 높일 수 있다??
- stochastic gradient가 batch gradient descent와 상당히 유사하지만, rather than waiting to sum up the gradient terms over all *m* examples, we take just one example and make progress in improving the parameters already
  - 루프의 마지막에 모든 데이터에 대한 업데이트하는 대신, 데이터에 대한 모든 단계에 대한 파라미터를 업데이트한다는 것을 의미한다.
- 파라미터를 위해 알고리즘은 무슨 일을 하는 가?
  - batch gradient descent는 global minimum을 찾는 일을 한다.
    - ![img](https://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[15].png)
  - stochastic gradient descent의 각 반복은 더 빨라지지만 각 반복은 하나의 example을 스쳐지나간다.
    - ![img](https://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[16].png)
      - 여기서 알 수 있는 것은 global minimum의 방향에서의 "일반적인" 움직임이지만, 항상 그렇지는 않다.
      - 절대 batch gradient descent가 그러는 것처럼 실제로 수렴되지 않는다. 그렇지만, 결국 global minimum과 가까운 어떤 지역에 떠돌게 된다.
        - 실제로는 이것은 문제가 되지 않는다 : minimum과 가까이 있다면 그것은 OK
- 마지막 implementation note
  - 아마도 전체 dataset을 1~10번 되는 loop가 필요할 것이다.
  - 만약 정말 큰 크기의 dataset을 가지고 있다면, dataset을 한 번 통과할 때 걸리는 시간에 이미 좋은 hypothesis를 가지고 있는 것이 가능할 것이다.
    - 이런 경우(m이 매우 매우 큰 경우)에 inner loop는 한 번만 실행해도 된다
- batch gradient descent와 대조한다면
  - 전체 dataset을 k번 통과시켜야 한다.  k는 데이터를 통과하기 위해 필요한 단계의 수

