# 01, 02. Regression Analysis, and Gradient Descent

---

#### 1. machine learning이란?

- Arthur Samuel(1959)
  - 컴퓨터에게 명확한 프로그램을 하지 않고 학습 능력을 제공하는 연구 분야
- Tom Michel(1999)
  - *"***A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."**

- learning algorithms의 유형
  - Supervised learning
    - 컴퓨터에 무언가 하는 방법을 가르쳐 주고 그 방법을 사용하도록 한다; 그 방법을 사용하여 새로운 지식을 얻는다.
  - Unsupervised learning
    - 컴퓨터가 무언가 하는 방법을 배울 수 있도록 한다. 그 방법으로 데이터에서의 구조와 패턴을 결정한다.
  - Reinforcement learning
  - Recommender systems

---

#### 2. Supervised learning - introduction

- 머신러닝의 가장 많은 문제 유형

  - ex) 집 값 예상

    - 집 값에 대한 데이터와 feet 사이즈와 얼마나 연관되어 있는 정도의 데이터 수집
    - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image.png)

    - 예상 문제!!
      - 이러한 데이터가 주어지고, 친구가 750$feet^2$의 집을 가지고 있다고 하면 얼마의 집 값을 예상할 수 있을까?

  - 접근 방법

    - 데이터를 통과한 직선
      - $ 150,000
    - 2차식
      - $ 200,000
    - (나중에 다룰 것)
      - 어떻게 직선인지 곡선인지를 결정할 것인가
    - 이러한 접근 방법들이 supervised learning을 하는 방법을 나타낸다

- 의미!!

  - 우리는 알고리즘에 "정답"이 제공되는 데이터 셋을 제공한다
  - 그래서 우리는 집에 대한 실제 가격을 알 수 있다.
    - *training data*에서 가격을 확실한 값으로 만드는 것을 배울 수 있다!
    - 그리고 알고리즘은 가격을 모르는 새로운 training data를 기반으로 더 정확한 정답을 만들어야 한다
    - *가격 예상*

- 또 이러한 문제를 **regression problem**이라고 한다.

  - 연속적인 출력값을 예측
  - 실제 이산 묘사 없음

- 또 다른 예시

  - 유방 암이 악성 암인지 양성 암인지 종양의 크기로 구분
  - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[1].png)
  - 데이터 살펴보기
    - 각 5개 존재
    - 종양의 크기로 예후를 측정할 수 있을까?
    - 이건 **classification problem**의 예시
      - 두 개의 이산적인 클래스에서 하나로 데이터를 분류하는 것
        - 두 클래스의 사이에 존재하는 것은 없음, 단순히 악성이나 아니냐
      - classification problem은 출력에 대해서 가능한 값의 이산 수를 갖는다.
        - 즉, 만약 4개의 값을 갖는다면
          - 0  "양성"
          - 1  "유형 1"
          - 2  "유형 2"
          - 3  "유형 4"
  - classification problems는 다른 방법으로 데이터를 plotting할 수 있다.
    - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[2].png)
      - 하나의 속성을 사용한 경우(size)

- 다른 문제에서는 여러 개의 attributes를 사용할 것

  - 예를 들어, 나이와 종양의 사이즈를 알고 있는 경우
    - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[3].png)

- 이 데이터를 기반으로 class를 나눌 수 있다.

  1. 두 개의 그룹 사이에 직선을 그린다.
  2. 두 그룹을 정의하기 위해서 더 복잡한 함수를 사용한다. (나중에 다룰 것)
  3. 개개인의 종양 사이즈와 나이를 알고 있을 때, class를 나누기 위한 정보로 사용할 수 있다.

- 고려해 볼 많은 features가 있을 수 있다.

  1. 종양의 두께
  2. 세포 크기의 균일성
  3. 세포 모양의 균일성

- 가장 exciting한 알고리즘은 무한개의 features를 다룰 수 있다.

  - 어떻게 무한개의 features를 다룰 수 있을까?
    - support vector machine으로 수학적 트릭을 간단하게 만든다 (나중에 다룰 것)
      - 만약 무한대로 긴 리스트를 가지고 있다면, 리스트를 가지고 알고리즘을 발전시킬 수 있다.

- *요약*

  - Supervised learning은 "올바른" 데이터를 갖도록 한다
    - Regression problem과 Classification problem에서

---

#### 3. Unsupervised learning - introduction

- Supervised learning 다음으로 자주 다뤄지는 문제 유형

- Unsupervised learning에서는 label이 붙지 않은 데이터를 갖는다.

  - 구조지을 수 있는 데이터 셋

- 구조짓는 방법의 하나는 데이터를 그룹으로 clustering하는 것이다.

  - **Clustering algorithm**

- *Clustering algorithm*

  - clustering algorithm의 예시
    - 구글 뉴스
      - 뉴스 스토리를 응집력 있는 그룹끼리 생성
    - 다른 문제들에서도 사용된다
      - 유전체학
      - microarray 데이터
        - 각 array마다 한 그룹을 가지고 있다.
        - 각 유전자의 척도마다
        - 개인을 어떤 유형의 사람으로 clustering하는 알고리즘을 실행
        - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[4].png)
      - 컴퓨터 clusters 구성하기
        - 잠재적 약점을 식별하거나 업무량을 효율적으로 분산
      - 소셜 네트워크 분석
        - 고객 정보
      - 우주 데이터 분석
        - 알고리즘은 놀라운 결과를 준다.
  - 기본적으로
    - 알아서 구조를 생성할 수 있는가?
    - 답을 제공하지 않기 때문에, unsupervised learning을 해야 한다

- *Cocktail party algorithm*

  - cocktail party problem

    - 많은 목소리가 섞여있다.

      - 모두가 말하는 것을 듣기가 어려움
      - 두 사람이 말하고 있다.
      - 마이크와 사람의 거리는 다르다
      - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[5].png)

    - 마이크가 어디에 있느냐에 따라 대화의 약간씩 다른 내용을 기록한다.

      - 목소리가 겹치는 것도 적지 않게 나타남

    - 각 마이크에서의 대화 기록을 가지고

      - cocktail party algorithm을 사용한다.

      - 알고리즘은 오디오 기록을 처리한다.

        1. 두 개의 오디오 소스가 있음을 결정

        2. 두 개의 소스를 분리한다.

    - 복잡한 문제인가?

      - 하나의 코드로 알고리즘 실행이 가능하다
      - ${[W, s, v] = svd((repmat(sum(x.*x, 1), size(x,1),1).*x)*x');}$ -> octave/MATLAB
        - 식별하기 어렵지만 프로그램은 짧ek

  - 알고리즘 이해

    - svd 
      - octave에 내장된 선형대수학
      - C++에서는 더 복잡

---

#### 4. Linear Regression

- 집 값 예상하는 예제
  - Supervised learning regression problem
- 무슨 값으로 시작??
  - training set = data set
  - 표기
    - m = training examples의 수
    - x's = input 변수 / features
    - y's = output 변수 "target" 변수
      - $(x,y)$ : 하나의 training example
      - $(x^i, y^j)$ : 특정 example ($i^{th}$ training example)
        - i는 training set의 index
- 정의된 training set을 가지고 어떻게 사용할 것인가
  - training set을 learning algorithm에 통과시킴
  - 알고리즘의 출력은 함수 $h$ (h = hypothesis)
    - 이 함수는 input을 가지고 동작 (새로운 집의 크기)
    - 추정된 y 값을 출력하려고 노력함
- hypothesis $h$는 어떻게 나타내는 가?
  - $h_\theta = \theta_0 + \theta_1x$
  - 간단하게 $h(x)$
- $h$함수의 의미
  - x에 대한 선형 함수 값은 y라는 의미
  - $\theta_i$는 파라미터
    - $\theta_0$는 0의 상태
    - $\theta_1$는 gradient
- 이런 함수의 종류는 하나의 변수를 갖는 linear regression
  - *univariate linear regression*
- 요약
  - hypothesis는 몇몇의 변수를 갖는다 ($\theta_0, \theta_1, x$)
  - learning system에 의해 결정된 parameters를 사용한다 ($\theta_i$)
  - input에 대한 예측을 출력한다

---

#### 5. Linear regression - implementation (cost function)

- cost function은 가지고 있는 데이터에 가장 잘 맞는 직선을 찾아내도록 한다
- $\theta_i$(파라미터) 값을 선택
  - 다른 값은 다른 함수를 만든다
  - 만약 $\theta_0 = 1.5$이고 $\theta_1 = 0$이면 $1.5 @ y$에 따른 X에 대해 평행한 직선을 갖는다
  - 만약 $\theta_1 > 0$이면 양수 기울기를 갖는다.
- training set에 기반하여 직선을 만드는 파라미터를 생성하고 싶다
  - 이러한 파라미터를 선택하여 $h_\theta(x)$가 training examples의 y와 비슷함
    - 기본적으로, 가능한 실제 y값과 비슷한 출력을 제공하는 $h_\theta(x)$에 training set의 x들을 사용한다.
    - $h_\theta(x)$가 *y의 모방*이라고 생각해보자
      - 이 함수는 x를 y로 바뀌도록 한다
      - 이미 y를 가지고 있음을 고려하고 $h_\theta(x)$가 x를 y로 바꾸는 일을 얼마나 잘 하는지 평가할 수 있다
- 공식화
  - 우리는 *minimization problem*을 해결하고 싶다
  - $(h_\theta(x) - y)^2$ 최소화
    - $h(x)$와 모든 각각의 example에 대한 y 사이의 차이를 최소화하는 것이다
  - 모든 training set에 대해 이 최소화한 차이를 합한다
  - $\frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$
    - 예상된 집 값과 실제 집 값 사이의 차이 제곱을 최소화한다
      - $\frac{1}{2m}$
        - $\frac{1}{m}$
          - 평균을 결정한다
        - $\frac{1}{2m}$
          - 약간 더 수학적 계산이 쉽도록 만든 것이며, 이미 정한 모든 상수는 변하지 않는다
          - 최솟값의 반의 여전히 최솟값!
      - $\theta_0/\theta_1$를 최소화한다는 것은 hypothesis 함수에 $\theta_0, \theta_1$를 사용했을 때 평균적으로 x의 *최소 편차*를 y에서 찾은 $\theta_0$과 $\theta_1$의 값을 얻는다는 것
  - cost function을 깔끔하게 정리한다면
    - $J(\theta_0, \theta_1) = \frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$
- cost function을 최소화하고 싶다
  - (summation항 때문에) cost function은 선천적으로 언제든지 training set에 대한 모든 데이터를 본다
- **다시 강조**
  - **Hypothesis**
    - prediction machine처럼 보임
    - x 값이 주어지고, 추정된 y 값을 얻는다
    - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[10].png)
  - **Cost**
    - training data를 사용하여 가능한 hypothesis가 정확하도록 만드는 $\theta$값을 결정하는 것이다
      - $minimize \; \theta_0,\theta_1 \; J(\theta_0, \theta_1)$
      - cost function은 squared error cost function
        - cost function은 대부분의 regression functions에 대해 적합한 선택이다
        - 아마도 흔히 쓰이는 함수이다
  - $J(\theta_0, \theta_1)$의 경우는 무슨 일을 하는지, 왜 이렇게 동작하는지 그리고 이후에 어떻게 사용될 지에 대해 약간 추상적이다.
- *Cost function - a deeper look*
  - cost function을 적용하는 방법과 왜 이 함수를 사용하는지에 대해 생각해보자
    - cost function은 파라미터를 결정한다
    - 파라미터와 관련된 값은 다른 값에 다른 결과를 생성하면서 어떻게 hypothesis가 동작하는지를 결정한다
  - 단순화한 hypothesis
    - $\theta_0 = 0$이라고 가정
    - $h_\theta(x) = \theta_1x$
    - $J(\theta_1) = \frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$
    - $minimize \; \theta_1 \; J(\theta_1)$
  - 여기서의 cost function과 목표는 $\theta_0$가 있을 때와 비슷하지만 파라미터가 좀 더 단순해졌다
    - 단순화한 hypothesis는 cost function J()을 시각화하는 게 좀 더 쉽다
  - 그래서 hypothesis는 0,0을 통과한다
  - 이해하고 싶은 두 개의 핵심 함수
    - $h_\theta(x)$
      - hypothesis는 x에 대한 함수
        - 집 크기가 몇인지에 대한 함수
    - $J(\theta_1)$
      - $\theta_1$ 파라미터에 대한 함수
    - 예제
      - $\theta_1 = 1$
      - $J(\theta_1) = 0$
    - 그래프에 그리기
      - $\theta_1$ vs $J(\theta_1)$
      - Data
        1. $\theta_1 = 1$  $J(\theta_1) = 0$
        2. $\theta_1 = 0.5$  $J(\theta_1) = $ ~0.58
        3. $\theta_1 = 0$  $J(\theta_1) = $ ~2.3
    - plot할 값의 범위를 계산한다면
      - $J(\theta_1)$ vs $\theta_1$ 다차식을 얻을 수 있다. (2차식으로 보임)
      - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[13].png)
  - learning algorithm에 대한 최적화 목적은 $J(\theta_1)$을 최소화하는 $\theta_1$의 값을 찾는 것
    - 그러므로, 여기 $\theta_1 = 1$은 $\theta_1$에 제일 좋은 값



---

#### 6. A deeper insight into the cost function - simplified cost function

- 등고선 plots 또는 등고선 figures에 익숙하자고 가정하고
  - 이전에 언급한 똑같은 cost function, hypothesis, 목표를 사용한다
- 2개의 변수가 있는 원래 복잡한 hypothesis를 사용한다
  - cost function : $$J(\theta_0, \theta_1)$$
- example
  - 만약 $$\theta_0 = 50$$,  $$\theta_1 = 0.06$$
  - 이전에 cost function으로 plotting함
    - $\theta_1$ vs $J(\theta_1)$
  - 이제 2개의 파라미터
    - plot은 약간 더 복잡해짐
    - 3D plot 생성
      - $X = \theta_1$
      - $Z = \theta_0$
      - $Y = J(\theta_0, \theta_1)$
      - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[14].png)
        - 높이(y)가 cost function의 값임을 알 수 있다. 그래서 y = $J(\theta_0, \theta_1)$가 최솟값인 곳을 찾아야 한다
- surface plot을 사용하는 대신 contour figures / plots를 사용할 수 있다
  - 각 다른 색의 타원이 그려져있다
  - 각각의 색은 $J(\theta_0, \theta_1)$의 같은 값이다. 하지만 $\theta_0$과 $\theta_1$이 다양할 것이기 때문에 명백하게 다른 위치에 plotting되어있다. 
  - bowl 모양의 함수가 화면에 보인다고 상상하면 가운데는 동심원이다.
  - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[15].png)
    - 각 x는 $\theta_0$와 $\theta_1$의 파라미터 값의 쌍을 나타낸다
      - 빨간색 x
        - $\theta_0$ = ~800
        - $\theta_1$ = ~-0.15
      - 좋은 위치가 아님
        - 이 파라미터들은 contour plot의 중심에서 멀리 떨어진 곳의 값을 가진다.
      - 만약
        - $\theta_0$ = ~360
        - $\theta_1$ = 0
        - 더 나은 hypothesis를 제공할 수 있지만 아직 좋은 것은 아님
          - contour plot의 중심이 아니기 때문
      - 최종적으로 우리는 best hypothesis를 제공하는 최솟값을 찾아야한다!
  - 눈과 손으로 하는 방법은 굉장히 골치 아픈 일
    - $\theta_0$과 $\theta_1$의 최솟값을 찾는 효육적인 알고리즘이 필요하다!!



---

#### 7. Gradient descent algorithm

- cost function J를 최소화한다

- Gradient descent

  - 최소화하는 모든 machine learning에서 사용된다

- 일반적인 J() 함수에서부터 시작한다

- 문제

  - $J(\theta_0, \theta_1)$를 가지고 있지만
  - $min \; J(\theta_0, \theta_1)$를 얻고 싶다

- Gradient descent는 더 일반화된 함수를 적용한다

  - $J(\theta_0, \theta_1, \theta_2,...,\theta_n)$
  - $min \; J(\theta_0, \theta_1, \theta_2,..., \theta_n)$

- *어떻게 동작하는가??*

  1. 초기 추측으로 시작한다

     - 0,0 혹은 또 다른 랜덤 값에서 시작한다

     - $\theta_0$, $\theta_1$를 조금씩 변경하면서 $J(\theta_0, \theta_1)$를 줄인다

  2. 파라미터가 변경될 때마다, 가능한 최대로 $J(\theta_0, \theta_1)$가 줄어드는 gradient를 선택한다

  3. 반복한다
  4. local minimum에 수렴할 때까지 진행한다

  - 흥미로운 특징
    - 시작할 곳에서는 어떤 최솟값으로 도달하는 것을 결정할 수 있다
    - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[16].png)
      - 여기 하나의 초기화된 점이 결국 하나의 local minimum으로 도달하는 것을 볼 수 있다.
      - 다른 점은 다른 곳으로 도달하였다

- *공식화된 정의*

  - 수렴할 때까지 진행한다
    - $\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_0, \theta_1) \;\;\; (for \; j=0 \; and \; j=1)$
  - 이 식의 의미
    - $(\theta_j - \alpha)$와 $\theta_j$에 대한 cost function의 편미분을 곱한 것을 적용해서 $\theta_j$를 업데이트한다
  - 표기법
    - :=
      - 할당한다는 표시
      - 주의, a = b는 참이라는 뜻
    - $\alpha$(alpha)
      - learning rate라고 부름
      - 어떤 크기의 스텝을 가질 것인지 제어함
        - $\alpha$가 크면 공격적인 gradient descent를 가질 것
          - loss가 줄어들지 않고 발산할 가능성이 있음
        - $\alpha$가 작으면 굉장히 작은 스텝을 가질 것
          - overfitting이 발생할 수 있으며, training time이 길어질 수 있음
  - 편미분항
    - $\frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1)$
      - 나중에 언급할 것
  - 어떻게 gradient descent algorithm이 적용되는지에 대한 부가적인 부분
    - $\theta_0$, $\theta_1$를 위한 동작
    - j = 0, j = 1에 대한 의미는 *동시에* 둘을 업데이트한다는 의미
    - 어떻게 동작?
      - 우항에 있는 $\theta_0, \theta_1$를 계산
        - 그래서 임시값이 필요
      - 그리고 같은 시간에 $\theta_0, \theta_1를 업데이트함
      - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[19].png)
  - 만약 동식에 업데이트하지 않는 것을 적용한다면 그것은 gradient descent가 아니고 이상하게 동작할 것
    - 근데 아마 제대로 동작하는 것처럼 보일 것이다 - 기억해둬야할 것!!

- *알고리즘 이해*

  - gradient descent를 이해하기 위해서 알고리즘을 더 자세히 설명할 수 있도록 하나의 파라미터를 최소화하는 더 단순한 함수로 만들어서 설명할 것이다
    - $min \; \theta_1 \; J(\theta_1)$ 에서 $\theta_1$은 실수다
  - 알고리즘에서 핵심적인 부분은
    - $\alpha$
    - 편미분항
  - 표기법 뉘양스
    - 편미분 vs 미분
      - 편미분을 사용하는 경우
        - 여러개의 변수를 가지고 있지만 하나에 대해 미분하는 경우
      - 미분을 사용하는 경우
        - 모든 변수에 대해 미분하는 경우
  - 미분항
    - $\frac{\partial}{\partial\theta_j}J(\theta_1)$
    - 미분항 가정
      - 각 점마다 탄젠트를 사용하고 선의 기울기를 보자
      - minimum(down)으로 움직이는 것은 음수 미분될 것이고, 알파는 항상 양수일 것이다. 그래서 $J(\theta_1)$은 더 작은 값으로 업데이트될 것이다
      - 비슷하게, 기울기가 더 위로 올라가려면 $J(\theta_1)$은 더 큰 숫자를 만들어야 한다
  - 알파 항($\alpha$)
    - 알파가 너무 크거나 너무 작을 때 나타나는 현상
    - 너무 작을 때
      - 정말 작은 스텝으로 움직임
      - 시간이 너무 오래 걸림
    - 너무 클 때
      - 최솟값을 지나칠 수 있음
      - 수렴에 실패할 가능성
  - local minimum을 얻었을 때
    - Gradient of tangent / derivative 는 0
    - 미분항 = 0
    - alpha * 0 = 0
    - $\theta_1 = \theta_1 - 0$  ;$\theta_1$은 똑같이 남음
  - globla minimum에 가까워질 수록 미분항은 더 작아진다. 그래서 더 작은 값을 업데이트할 수 있다. 알파가 고정되어 있어도
    - 최솟값에 가까워지기 위해 알고리즘이 더 작은 스텝으로 동작한다는 의미
    - 알파를 시간이 지날수록 변경해주어야 한다'



---

#### 8. Linear regression with gradient descent

- squared error cost functin $J(\theta_0, \theta_1)$를 최소화하기 위해 gradient descent를 적용할 것이다.
- 이제 미분항을 가질 것
  - $\frac{\partial}{\partial\theta_j}J(\theta_0, \theta_1) = \frac{\partial}{\partial\theta_j}\frac{1}{2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2 = \frac{\partial}{\partial\theta_j}\frac{1}{2m}\sum\limits_{i=1}^{m}(\theta_0+\theta_1x^{(i)}-y^{(i)})^2$
- 단지 첫 번째 식을 확장한 것
  - $J(\theta_0, \theta_1) = \frac{1}{2m}$
  - $h_\theta(x) = \theta_0 + \theta_1x$
- 각 파라미터마다의 미분을 결정
  - j = 0일 때
  - j = 1일 때
- $\theta_0, \theta_1$인 경우에 편미분이 무엇인지 알아야 함
  - j = 0이고 j = 1일 때 이러한 식을 이끌어내려면 이 식을 얻어야 함
    - $j = 0 : \frac{\partial}{\partial\theta_0}J(\theta_0, \theta_1) = \frac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})$
    - $j = 1 : \frac{\partial}{\partial\theta_0}J(\theta_0, \theta_1) = \frac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)}$
- 위의 식을 확인하기 위해서는 여러 개 변수들의 계산을 할 수 있어야 한다
  - 그리고 우리는 이런 값을 gradient descent algorithm에 다시 대입할 수 있다
- 어떻게 동작하는가?
  - 다른 local optinum을 만난다는 위험
  - linear regression cost function은 항상 convex function이다 (항상 하나의 최솟값을 갖음)
    - Bowl 형태
    - 하나의 global optima
      - gradient descent는 항상 global optima로 수렴한다
  - 진행중에는
    - 값 초기화
      - $\theta_0=900$
      - $\theta_1=-0.1$
- ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[23].png)
  - 결국 global minimum에 도달한다
- 사실 이건 Batch Gradient Descent다
  - 각 스텝마다 모든 training data를 살펴본다는 사실을 참조하자
    - 각 스텝마다 m개의 training examples를 계산한다
  - 가끔 작은 데이터 subsets를 사용하는 non-batch 버전도 있지만
    - gradient descent의 또 다른 형태를 나중에 살펴볼 것 (m이 굉장히 클 때)
- minimum function으로 해결 방법을 찾는 여러 가지 방법이 있다
  1. 표준 방정식 (Normal equations)
  2. 더 큰 데이터 셋을 가지고 더 좋은 Gradient descent를 사용하는 것
  3. 많은 contexts와 머신러닝을 사용한다



---

*다음에 배울 것 - 중요한 확장*

​	알고리즘에서 확장된 2가지

1.  평균 방정식 : 수학적 해결방법
    - 최소화 문제를 풀기 위해서 gradient descent를 사용하는 반복적인 접근 방법을 피하는 수학적인 방법을 사용하는 $[min \; J(\theta_0, \theta_1)]$를 해결할 수 있다
    - 장점
      - $\alpha$항이 없다
      - 어떤 문제에서는 더 빠르게 해결할 수 있다
    - 단점
      - 훨씬 더 복잡하다
    - 여러 개의 features를 갖는 linear regression을 배우는 부분에서 표준 방정식에 대해 다시 언급할 것
2.  많은 양의 features를 학습하는 것
    - 많은 features가 있다면 목적에 도달할 수 있도록 기여하는 다른 파라미터들이 있을 수 있다
      - 집을 예시로 들자면
        - 집의 크기
        - 집의 연식
        - 침실의 개수
        - 층수
      - x1, x2, x3, x4
    - 여러 개의 features가 있으면 plotting하기 어렵다
      - 3차원이 넘어가면 실제로 plotting을 하지 않는다
      - 표기도 훨씬 복잡해진다
        - 표기하는 제일 좋은 방법은 선형대수를 이용하는 것
        - 행렬과 벡터로 나타낼 수 있다
        - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[24].png)
          - 이 행렬은 집의 크기와 침실의 개수, 층수, 집의 연식을 보여준다
          - 하나의 변수에 모든 것이 들어가 있다
            - 숫자 블록, 모든 데이터를 하나의 큰 블록으로 만든다
    - 벡터 (Vector)
      - ![img](https://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[25].png)
      - y를 보여줌
      - 집 가격을 보여줌
    - 더 복잡한 linear regression models를 위해서는 선형대수가 필요하다
    - 선형대수는 계산적으로 효율적인 모델을 만들기에 좋다
      - 크기가 큰 데이터 셋의 동작이 잘 되는 방법을 제공한다
      - 보통 벡터화 문제는 흔한 optimization 기술이다
